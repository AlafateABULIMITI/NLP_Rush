{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Web_mining_lab.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlafateABULIMITI/NLP_Rush/blob/master/Web_mining_lab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkvxNKvtCO9u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "outputId": "e6dac995-1a50-4d16-fda5-522341799da2"
      },
      "source": [
        "%%shell\n",
        "wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1sJzVrwDZyWvmKFGS2YH1Xg4MyVtbSZf3' -O data.zip\n",
        "unzip data"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-12-12 12:15:42--  https://docs.google.com/uc?export=download&id=1sJzVrwDZyWvmKFGS2YH1Xg4MyVtbSZf3\n",
            "Resolving docs.google.com (docs.google.com)... 74.125.195.102, 74.125.195.113, 74.125.195.139, ...\n",
            "Connecting to docs.google.com (docs.google.com)|74.125.195.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-0g-ac-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/4lehja9gg8j91vhnd8ub5erpaa0rejjd/1576152000000/01123303726322883128/*/1sJzVrwDZyWvmKFGS2YH1Xg4MyVtbSZf3?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2019-12-12 12:15:42--  https://doc-0g-ac-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/4lehja9gg8j91vhnd8ub5erpaa0rejjd/1576152000000/01123303726322883128/*/1sJzVrwDZyWvmKFGS2YH1Xg4MyVtbSZf3?e=download\n",
            "Resolving doc-0g-ac-docs.googleusercontent.com (doc-0g-ac-docs.googleusercontent.com)... 74.125.142.132, 2607:f8b0:400e:c08::84\n",
            "Connecting to doc-0g-ac-docs.googleusercontent.com (doc-0g-ac-docs.googleusercontent.com)|74.125.142.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/zip]\n",
            "Saving to: ‘data.zip’\n",
            "\n",
            "\rdata.zip                [<=>                 ]       0  --.-KB/s               \rdata.zip                [ <=>                ]   4.01M  18.1MB/s               \rdata.zip                [  <=>               ]   7.21M  30.0MB/s    in 0.2s    \n",
            "\n",
            "2019-12-12 12:15:42 (30.0 MB/s) - ‘data.zip’ saved [7555220]\n",
            "\n",
            "Archive:  data.zip\n",
            "   creating: logs/\n",
            "  inflating: __MACOSX/._logs         \n",
            "  inflating: logs/.DS_Store          \n",
            "  inflating: __MACOSX/logs/._.DS_Store  \n",
            "  inflating: logs/csti.txt           \n",
            "  inflating: __MACOSX/logs/._csti.txt  \n",
            "  inflating: logs/fne.asso.txt       \n",
            "  inflating: __MACOSX/logs/._fne.asso.txt  \n",
            "  inflating: logs/bourges.txt        \n",
            "  inflating: __MACOSX/logs/._bourges.txt  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oea6ZgUqCR_G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "outputId": "534eef65-ef79-4286-9ef6-6fca273793b5"
      },
      "source": [
        "##################################\n",
        "# Step 1\n",
        "# Import log file as a dataframe\n",
        "##################################\n",
        "verbose = True\n",
        "\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "if verbose:\n",
        "    print(\"Parsing log file into Pandas dataframe\")\n",
        "\n",
        "IP = []\n",
        "ID = []\n",
        "DATE = []\n",
        "URL = []\n",
        "STATUS = []\n",
        "SENT = []\n",
        "REF = []\n",
        "AGENT = []\n",
        "\n",
        "path = \"../content/logs/bourges.txt\"\n",
        "pattern = re.compile(\n",
        "    '^([^ ]+) ([^ ]+) [^ ]+ \\[([^\\]]+)\\] \"([^\"]+)\" ([0-9]{3}) ([^ ]+) \"([^\"]+)\" \"([^\"]+)\"$'\n",
        ")\n",
        "fields = [\"IP\", \"ID\", \"DATE\", \"URL\", \"STATUS\", \"SENT\", \"REF\", \"AGENT\"]\n",
        "\n",
        "with open(path, \"r\") as f:\n",
        "    for line in f:\n",
        "        g = re.match(pattern, line).groups()\n",
        "        IP.append(g[0])\n",
        "        ID.append(g[1])\n",
        "        DATE.append(g[2])\n",
        "        URL.append(g[3])\n",
        "        STATUS.append(g[4])\n",
        "        SENT.append(g[5])\n",
        "        REF.append(g[6])\n",
        "        AGENT.append(g[7])\n",
        "cols = zip(IP, ID, DATE, URL, STATUS, SENT, REF, AGENT)  # NIL,\n",
        "df = pd.DataFrame(cols, columns=fields)\n",
        "df.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parsing log file into Pandas dataframe\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>IP</th>\n",
              "      <th>ID</th>\n",
              "      <th>DATE</th>\n",
              "      <th>URL</th>\n",
              "      <th>STATUS</th>\n",
              "      <th>SENT</th>\n",
              "      <th>REF</th>\n",
              "      <th>AGENT</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>crawler10.googlebot.com</td>\n",
              "      <td>-</td>\n",
              "      <td>05/Sep/2004:04:26:24 +0200</td>\n",
              "      <td>GET /html/rhinolophides.html HTTP/1.0</td>\n",
              "      <td>304</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>Googlebot/2.1 (+http://www.google.com/bot.html)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>crawler13.googlebot.com</td>\n",
              "      <td>-</td>\n",
              "      <td>05/Sep/2004:04:30:00 +0200</td>\n",
              "      <td>GET /html/jeunesse.htm HTTP/1.0</td>\n",
              "      <td>304</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>Googlebot/2.1 (+http://www.google.com/bot.html)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>crawler14.googlebot.com</td>\n",
              "      <td>-</td>\n",
              "      <td>05/Sep/2004:04:31:22 +0200</td>\n",
              "      <td>GET / HTTP/1.0</td>\n",
              "      <td>304</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>Googlebot/2.1 (+http://www.google.com/bot.html)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>crawler11.googlebot.com</td>\n",
              "      <td>-</td>\n",
              "      <td>05/Sep/2004:04:32:20 +0200</td>\n",
              "      <td>GET /anglais/html/index_studies.htm HTTP/1.0</td>\n",
              "      <td>200</td>\n",
              "      <td>700</td>\n",
              "      <td>-</td>\n",
              "      <td>Googlebot/2.1 (+http://www.google.com/bot.html)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>crawler14.googlebot.com</td>\n",
              "      <td>-</td>\n",
              "      <td>05/Sep/2004:04:47:23 +0200</td>\n",
              "      <td>GET /html/non_exclu.htm HTTP/1.0</td>\n",
              "      <td>304</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>Googlebot/2.1 (+http://www.google.com/bot.html)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                        IP  ...                                            AGENT\n",
              "0  crawler10.googlebot.com  ...  Googlebot/2.1 (+http://www.google.com/bot.html)\n",
              "1  crawler13.googlebot.com  ...  Googlebot/2.1 (+http://www.google.com/bot.html)\n",
              "2  crawler14.googlebot.com  ...  Googlebot/2.1 (+http://www.google.com/bot.html)\n",
              "3  crawler11.googlebot.com  ...  Googlebot/2.1 (+http://www.google.com/bot.html)\n",
              "4  crawler14.googlebot.com  ...  Googlebot/2.1 (+http://www.google.com/bot.html)\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNmdLDizC2Uk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "d62ce20d-9133-4235-f710-2acaf9aadd8a"
      },
      "source": [
        "##################################\n",
        "# Step 2\n",
        "# Convert date string to datetime format\n",
        "# Remove get/post and HTTP/ from URLs\n",
        "##################################\n",
        "if verbose:\n",
        "    print(\"Parsing date string into datetime\")\n",
        "# Changing to datetime\n",
        "import datetime as dt\n",
        "import pytz\n",
        "\n",
        "timezone_nw = pytz.timezone(\"Etc/GMT-2\")\n",
        "\n",
        "dateformat = \"%d/%b/%Y:%H:%M:%S\"\n",
        "corrected_date = []\n",
        "for date_time_str in df[\"DATE\"]:\n",
        "    date_short = date_time_str.split(\" \")[0]\n",
        "    datetime = dt.datetime.strptime(date_short, dateformat)  # import from date format\n",
        "    final_date = datetime.astimezone(timezone_nw)  # change the timezone to +2:00\n",
        "    corrected_date.append(final_date)  # build a list of datetime\n",
        "df[\"DATE\"] = corrected_date\n",
        "\n",
        "urlformat = \"^(GET|POST|HEAD) ([^ ]+) (HTTP.*)$\"\n",
        "urlpattern = re.compile(urlformat)\n",
        "newurls = []\n",
        "for url_str in df[\"URL\"]:\n",
        "    if re.match(urlpattern, url_str):\n",
        "        g = re.match(urlpattern, url_str).groups()\n",
        "        newurls.append(g[1].lower())\n",
        "    else:\n",
        "        newurls.append(url_str.lower()[0:128])\n",
        "df[\"URL\"] = newurls\n",
        "print(df.head(5))\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parsing date string into datetime\n",
            "                        IP  ...                                            AGENT\n",
            "0  crawler10.googlebot.com  ...  Googlebot/2.1 (+http://www.google.com/bot.html)\n",
            "1  crawler13.googlebot.com  ...  Googlebot/2.1 (+http://www.google.com/bot.html)\n",
            "2  crawler14.googlebot.com  ...  Googlebot/2.1 (+http://www.google.com/bot.html)\n",
            "3  crawler11.googlebot.com  ...  Googlebot/2.1 (+http://www.google.com/bot.html)\n",
            "4  crawler14.googlebot.com  ...  Googlebot/2.1 (+http://www.google.com/bot.html)\n",
            "\n",
            "[5 rows x 8 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDvwoXgADBpg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "2bb19f5a-a51d-49a0-d0f2-c790e12b4ffa"
      },
      "source": [
        "##################################\n",
        "# Step 3\n",
        "# Build sessions and urls list\n",
        "##################################\n",
        "if verbose:\n",
        "    print(\"Building sessions and urls dictionary\")\n",
        "\n",
        "urls = []  # URLS list: assign a unique ID to each URL\n",
        "sessions = {}  # key: id_session, value = {id_user, [url], [timestamp]}\n",
        "tmp = {}  #\n",
        "nb_sessions = 0\n",
        "for index, entry in df.iterrows():\n",
        "    # step 3.1. Retrieving / encoding the URL\n",
        "    if not (entry[\"URL\"] in urls):\n",
        "        urls.append(entry[\"URL\"])\n",
        "    # step 3.2. Building a new session or extending an existing session\n",
        "    # Current on-going sessions are stored in tmp\n",
        "    if entry[\"IP\"] in tmp:  # scanning keys of tmp dict to find the same user\n",
        "        u, t = tmp.get(entry[\"IP\"])  # reading tuple([urls], [timestamps])\n",
        "        # comparing last timestamp with new one in seconds\n",
        "        diff = (entry[\"DATE\"] - t[-1]).total_seconds()\n",
        "        if diff <= (60 * 30):  # if elapsed time <= 30 minutes\n",
        "            # we are in the same session for user entry[\"IP\"]\n",
        "            u.append(urls.index(entry[\"URL\"]))\n",
        "            t.append(entry[\"DATE\"])\n",
        "            tmp[entry[\"IP\"]] = (u, t)  # update session for entry[\"IP\"]\n",
        "        else:\n",
        "            # a. saving current session from tmp into sessions\n",
        "            ses = (entry[\"IP\"], u, t)\n",
        "            sessions[nb_sessions] = ses\n",
        "            nb_sessions += 1\n",
        "\n",
        "            # b. starting a new session for user entry[\"IP\"]\n",
        "            tmp[entry[\"IP\"]] = ([urls.index(entry[\"URL\"])], [entry[\"DATE\"]])\n",
        "\n",
        "    else:  # building new entry in tmp\n",
        "        tmp[entry[\"IP\"]] = ([urls.index(entry[\"URL\"])], [entry[\"DATE\"]])\n",
        "\n",
        "# step 3.3 moving all sessions remaining in tmp to sessions\n",
        "for key, values in tmp.items():\n",
        "    sessions[nb_sessions] = (\n",
        "        key,\n",
        "        values[0],\n",
        "        values[1],\n",
        "    )  # here key = IP, values[0] = [urls], values[1] = [timestamps]\n",
        "    nb_sessions += 1\n",
        "\n",
        "# printing some statistics\n",
        "print(\"Number of sessions: \", len(sessions), \"Number of urls: \", len(urls))\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building sessions and urls dictionary\n",
            "Number of sessions:  2036 Number of urls:  404\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2DmeD1pDEZe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "9120c101-0909-450f-b861-11b92864bddd"
      },
      "source": [
        "##################################\n",
        "# Step 4\n",
        "# Build a dataset where one row = one vector of sessions\n",
        "# as a numpy array\n",
        "##################################\n",
        "import numpy as np\n",
        "\n",
        "if verbose:\n",
        "    print(\"Building Numpy structure with vectorized sessions\")\n",
        "\n",
        "# build the correct sized array\n",
        "data = np.zeros((len(sessions), len(urls)))\n",
        "\n",
        "# scan the sessions to build the data matrix\n",
        "i = 0\n",
        "for id_user, list_urls, list_timestamps in sessions.values():\n",
        "    for u in list_urls:  # u values are already indexes of the urls in the list\n",
        "        data[i][u] += 1\n",
        "    i += 1\n",
        "\n",
        "print(data[range(0, 5)])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building Numpy structure with vectorized sessions\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 1. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYiQg2bcDGl-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "outputId": "f320b221-d0a1-4a91-ef6a-60c1b39fbc65"
      },
      "source": [
        "##################################\n",
        "# Step 5\n",
        "# Analyze the web session by clustering\n",
        "##################################\n",
        "\n",
        "if verbose:\n",
        "    print(\"Applying clustering approaches\")\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# nclusters = 7\n",
        "for nclusters in range(3, 11):\n",
        "    kmeans = KMeans(n_clusters=nclusters, random_state=0).fit(data)\n",
        "    print(\"Size of discovered clusters: \", np.bincount(kmeans.labels_))\n",
        "    s = silhouette_score(data, kmeans.labels_)\n",
        "    print(\"Quality of clustering with\", nclusters, \" clusters:\", s)\n",
        "\n",
        "# distance are accessible through:  https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html\n",
        "# clustering algorithms: https://scikit-learn-extra.readthedocs.io/en/latest/generated/sklearn_extra.cluster.KMedoids.html\n",
        "# to begin with a simple k-means : https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Applying clustering approaches\n",
            "Size of discovered clusters:  [2006   29    1]\n",
            "Quality of clustering with 3  clusters: 0.8296672877873453\n",
            "Size of discovered clusters:  [1217  798    1   20]\n",
            "Quality of clustering with 4  clusters: 0.481721351725549\n",
            "Size of discovered clusters:  [1215    8    1  771   41]\n",
            "Quality of clustering with 5  clusters: 0.48766752733448365\n",
            "Size of discovered clusters:  [  20 1215    8  706    1   86]\n",
            "Quality of clustering with 6  clusters: 0.48177197116836956\n",
            "Size of discovered clusters:  [1213    8  170    1  544   80   20]\n",
            "Quality of clustering with 7  clusters: 0.4661483833067375\n",
            "Size of discovered clusters:  [ 588    3 1215    1   82   17  125    5]\n",
            "Quality of clustering with 8  clusters: 0.4737355537584814\n",
            "Size of discovered clusters:  [   4 1215    1   18  587   78    6  125    2]\n",
            "Quality of clustering with 9  clusters: 0.4759445947343963\n",
            "Size of discovered clusters:  [1214   79  586    1    6    4    2    3  124   17]\n",
            "Quality of clustering with 10  clusters: 0.47619686799673583\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgjtNMEQDI7u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}